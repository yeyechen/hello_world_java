package 左程云体系学习班;

public class Lecture33 {
  /*
   * 哈希函数。
   * （不要求代码实现，面试中只会聊抽象概念）
   * 1. 认识哈希函数：
   * (a) 输入参数，假设是int类型，可能性无穷大。
   * (b) 输出参数，可能性可以很大但一定是有穷尽的。
   * (c) 哈希函数没有随机机制，固定的输入一定是固定的输出
   * (d) 输入无穷多但是输出值有限，所以不同输入也可能有相同输出，这种情况被称为"哈希碰撞"。
   * (e) 哈希函数的输出值会几乎平均地被分布在输出域上。
   *
   * 2. 哈希表的设计
   * 将一个pair(key, value)放入一个哈希表时，会有一个哈希桶，桶会有一个长度，比如17。那么放入之前用哈希函数将
   * key算出一个哈希值，再 mod 17。的出来的数放入哈希桶对应的位置上。当多个pairs被放入桶中时，肯定会均匀分布在0～17
   * 上，因为(e)性质。但是总有可能哈希碰撞，即同一个位置有多个pairs。将他们用linked list的方式连接。找的时候也一样，
   * 因为哈希函数的性质相同输入会有相同输出，用哈希函数算一次再mod 17，我们就知道去哪里找这个value。如果有多个pair，
   * 遍历。当某一个位置的linked list长度到达人为规定较慢的长度时，比如说6，我们将桶进行扩容比如2倍，新桶大小为34。
   * 原桶中所有的数据都重新算哈希值重新mod 34，重新入桶。这样从而达到O(1)的增删改查。新桶中各个位置链表的长度会约为
   *
   * 3. 布隆过滤器(Bloom Filter)
   * 我们想要过滤一些黑名单网站，比如说100亿个黑名单网站，我们向服务器发送一个网站请求时会通过一些过滤器阻止你进入
   * 一些网站，布隆过滤器就是这样的设计。布隆过滤器的最大优点是极其省空间，并且带来的失误率能被降低到一个可观的程度。
   * 准备一个bit的数组，可以用int[]来代替，每一个int都是32位bit的数组。准备m位数的bit数组，准备k个哈希函数，
   * 拿一个黑名单网站，各走一遍这k个哈希函数，mod m，在bit数组中对应的数字设置位1。那么如果下次我们向服务器请求这个
   * 黑色网站的话，服务器也会拿这个网站的string，各走一次k个哈希函数，并mod m。如果这k个结果都为1，则判定为黑色网站
   * 请求，拒绝访问。那么拿这100亿个网站，都通过这k个哈希函数存入bit数组中，耗费的空间也仅仅只有17个G。当然布隆过滤器
   * 是有失误率的，但只会有false positive，比如请求一个正常网站，被过滤器判定为黑色网站。
   * 那么布隆过滤器的各个参数都是可以通过公式计算的。样本大小(黑名单网站数量)为 n，预期失误率为 p，那么bit数组大小 m
   * 就为：m= - \frac{n*ln(p)}{ln(2)^2}
   * 哈希函数数量 k = ln(2) * m/n ，因为k为整数，向上取整。
   * 真实失误率 p = (1 - \exp{-\frac{nk}{m}})^k
   * 如果面试中遇到类似过滤器的题目，首先询问是否容忍失误率，如果是，考察的就是布隆过滤器。
   *
   * 4. 一致性哈希。有k个机器，举例分别有三个机器m1，m2，m3，然后处理负载。给定一个哈希算法f，把输出域想象成一个环，
   * 每进来一个数据，通过哈希算法算出后给到环上顺时针下一个机器。（每个机器都有自己的哈希值）
   * ...------m1------m2------m3------m1------...
   * 那么通过哈希的一致性，m1，m2，m3分配到的负载是相同的。现在出现了一个问题，就是我们要新增机器，或是删减机器，会
   * 变得很复杂，因为加到哪儿都不能让负载均衡了。那么还牵扯到数据迁移的问题，如果数据要重新大洗牌就太麻烦了。
   * 有一个解决办法就是虚拟节点。给m1，m2，m3各随机分配1000个数：
   * m1: [a1, a2, ..., a1000]
   * m2: [b1, b2, ..., b1000]
   * m3: [c1, c2, ..., c1000]
   * 那么这3000个数通过哈希算法f能大致均匀地分配到输出域中去。
   * 此时新增了一个机器，m4，同样给它随机分配1000个数：
   * m4: [d1, d2, ..., d1000]
   * 那么这1000个属于m4的数也同样被均匀地分配到输出域中去了。此时再来一个服务器请求，各个机器被分配到该请求的概率
   * 则变为了四分之一。
   * 对于数据迁移，比如人为规定，新机器的哈希数的下一个旧机器哈希数的数据，迁移到新的数中去。那么就保证了每个旧机器
   * 几乎均等地把各个节点的数据给到了新机器。
   *  */

}
